import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
from sklearn.manifold import TSNE
import os
import glob
import re

# 导入自定义模块 (假设前两个代码片段已保存为 Python 模块)
# from cnn_lstm_attention_model import create_cnn_lstm_attention_model
# from data_preprocessing import (load_and_process_data, segment_data, 
#                               normalize_features, data_augmentation)

def create_label_mapping():
    """
    创建故障类型和标签的映射
    
    返回:
    - 标签映射字典
    """
    # 故障类型
    fault_types = ['healthy', 'inner', 'outer']
    
    # 故障严重程度
    severities = ['0.0', '0.7', '0.9', '1.1', '1.3', '1.5', '1.7']  # 0.0用于健康状态
    
    # 负载条件
    loads = ['100W', '200W', '300W']
    
    # 创建映射
    label_map = {}
    label_count = 0
    
    # 健康状态 (无故障)
    for load in loads:
        for pulley in ['with_pulley', 'without_pulley']:
            key = f"healthy_{load}_{pulley}"
            label_map[key] = label_count
            label_count += 1
    
    # 故障状态
    for fault in ['inner', 'outer']:
        for severity in severities[1:]:  # 跳过0.0
            for load in loads:
                key = f"{fault}_{severity}mm_{load}"
                label_map[key] = label_count
                label_count += 1
    
    return label_map

def parse_filename(filename):
    """
    从文件名解析故障类型、严重程度和负载信息
    
    参数:
    - filename: 文件名
    
    返回:
    - 解析后的标签
    """
    # 这里需要根据您的实际文件命名规则进行调整
    # 例如: "0.7inner100watt67V2Iv.csv" 表示0.7mm内圈故障，100W负载
    
    # 提取故障类型和严重程度
    if 'healthy' in filename.lower():
        fault_type = 'healthy'
        severity = '0.0'
    elif 'inner' in filename.lower():
        fault_type = 'inner'
        # 提取数字作为严重程度
        severity_match = re.search(r'(\d+\.\d+)', filename)
        severity = severity_match.group(1) if severity_match else 'unknown'
    elif 'outer' in filename.lower():
        fault_type = 'outer'
        # 提取数字作为严重程度
        severity_match = re.search(r'(\d+\.\d+)', filename)
        severity = severity_match.group(1) if severity_match else 'unknown'
    else:
        fault_type = 'unknown'
        severity = 'unknown'
    
    # 提取负载信息
    if '100w' in filename.lower() or '100watt' in filename.lower():
        load = '100W'
    elif '200w' in filename.lower() or '200watt' in filename.lower():
        load = '200W'
    elif '300w' in filename.lower() or '300watt' in filename.lower():
        load = '300W'
    else:
        load = 'unknown'
    
    # 确定是否有皮带轮
    pulley = 'with_pulley' if 'pulley' in filename.lower() else 'without_pulley'
    
    # 组合成标签
    if fault_type == 'healthy':
        label = f"healthy_{load}_{pulley}"
    else:
        label = f"{fault_type}_{severity}mm_{load}"
    
    return label

def load_and_prepare_dataset(data_dir, label_map, window_size=1000, overlap=0.5):
    """
    加载并准备数据集
    
    参数:
    - data_dir: 数据目录
    - label_map: 标签映射
    - window_size: 窗口大小
    - overlap: 重叠比例
    
    返回:
    - X: 特征数据
    - y: 独热编码后的标签
    """
    X = []
    y = []
    
    # 获取所有CSV文件
    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
    
    for file_path in csv_files:
        filename = os.path.basename(file_path)
        
        # 解析文件名获取标签
        label_str = parse_filename(filename)
        
        # 如果标签在映射中存在
        if label_str in label_map:
            label = label_map[label_str]
            
            try:
                # 加载数据
                data = load_and_process_data(file_path)
                
                # 分割数据
                segments = segment_data(data, window_size, overlap)
                
                # 数据增强（可选）
                # segments = data_augmentation(segments)
                
                # 添加到数据集
                X.extend(segments)
                y.extend([label] * len(segments))
                
                print(f"处理文件: {filename}, 标签: {label_str}, 添加了 {len(segments)} 个样本")
            except Exception as e:
                print(f"处理文件 {filename} 时出错: {str(e)}")
    
    # 转换为numpy数组
    X = np.array(X)
    y = np.array(y)
    
    # 独热编码标签
    y_categorical = to_categorical(y, num_classes=len(label_map))
    
    return X, y_categorical

def train_and_evaluate_model(data_dir):
    """
    训练并评估模型
    
    参数:
    - data_dir: 数据目录
    
    返回:
    - 训练好的模型和评估结果
    """
    # 创建标签映射
    label_map = create_label_mapping()
    
    # 加载并准备数据集
    X, y = load_and_prepare_dataset(data_dir, label_map)
    
    print(f"数据集大小: {X.shape}")
    print(f"标签数量: {y.shape}")
    
    # 分割训练、验证和测试集
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
    
    print(f"训练集大小: {X_train.shape}")
    print(f"验证集大小: {X_val.shape}")
    print(f"测试集大小: {X_test.shape}")
    
    # 创建模型
    input_shape = (X.shape[1], X.shape[2])  # (窗口大小, 特征数)
    num_classes = len(label_map)
    
    model, attention_weights = create_cnn_lstm_attention_model(input_shape, num_classes)
    
    # 打印模型摘要
    model.summary()
    
    # 训练模型
    history = train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32)
    
    # 评估模型
    loss, accuracy, cm = evaluate_model(model, X_test, y_test)
    
    # 可视化混淆矩阵
    visualize_confusion_matrix(cm, list(label_map.keys()))
    
    # 可视化学习曲线
    visualize_learning_curves(history)
    
    # 可视化潜在空间（使用t-SNE）
    visualize_latent_space(model, X_test, y_test, num_classes)
    
    # 可视化注意力权重
    visualize_attention_weights(X_test, attention_weights, model)
    
    return model, history, (loss, accuracy)

def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):
    """
    训练模型
    
    参数:
    - model: 创建的模型
    - X_train, y_train: 训练数据
    - X_val, y_val: 验证数据
    - epochs: 训练轮数
    - batch_size: 批次大小
    
    返回:
    - 训练历史
    """
    # 早停法
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )
    
    # 学习率衰减
    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6
    )
    
    # 模型检查点
    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        'best_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max'
    )
    
    # 训练模型
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, lr_scheduler, checkpoint]
    )
    
    return history

def evaluate_model(model, X_test, y_test):
    """
    评估模型
    
    参数:
    - model: 训练好的模型
    - X_test, y_test: 测试数据
    
    返回:
    - 测试损失和准确率以及混淆矩阵
    """
    # 评估模型
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"测试损失: {loss:.4f}")
    print(f"测试准确率: {accuracy:.4f}")
    
    # 预测
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)
    
    # 计算混淆矩阵
    cm = confusion_matrix(y_true_classes, y_pred_classes)
    
    # 打印分类报告
    print("\n分类报告:")
    print(classification_report(y_true_classes, y_pred_classes))
    
    return loss, accuracy, cm

def visualize_confusion_matrix(cm, class_names):
    """
    可视化混淆矩阵
    
    参数:
    - cm: 混淆矩阵
    - class_names: 类别名称
    """
    plt.figure(figsize=(20, 16))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('预测')
    plt.ylabel('真实')
    plt.title('混淆矩阵')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300)
    plt.show()

def visualize_learning_curves(history):
    """
    可视化学习曲线
    
    参数:
    - history: 训练历史
    """
    plt.figure(figsize=(12, 5))
    
    # 绘制损失曲线
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='训练损失')
    plt.plot(history.history['val_loss'], label='验证损失')
    plt.title('损失曲线')
    plt.xlabel('轮数')
    plt.ylabel('损失')
    plt.legend()
    
    # 绘制准确率曲线
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='训练准确率')
    plt.plot(history.history['val_accuracy'], label='验证准确率')
    plt.title('准确率曲线')
    plt.xlabel('轮数')
    plt.ylabel('准确率')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('learning_curves.png', dpi=300)
    plt.show()

def visualize_latent_space(model, X_test, y_test, num_classes):
    """
    使用t-SNE可视化潜在空间
    
    参数:
    - model: 训练好的模型
    - X_test: 测试数据
    - y_test: 测试标签
    - num_classes: 类别数量
    """
    # 创建中间层模型以获取潜在表示
    # 假设LSTM的输出是潜在表示
    intermediate_layer_model = tf.keras.Model(
        inputs=model.input,
        outputs=model.layers[-4].output  # 假设倒数第4层是包含潜在表示的层
    )
    
    # 获取潜在表示
    latent_representations = intermediate_layer_model.predict(X_test)
    
    # 使用t-SNE降维
    tsne = TSNE(n_components=2, random_state=42)
    tsne_results = tsne.fit_transform(latent_representations)
    
    # 获取真实类别
    y_true = np.argmax(y_test, axis=1)
    
    # 绘制t-SNE结果
    plt.figure(figsize=(12, 10))
    cmap = plt.cm.get_cmap('tab20', num_classes)
    
    for i in range(num_classes):
        indices = y_true == i
        plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], c=[cmap(i)], label=f'Class {i}', alpha=0.7)
    
    plt.title('t-SNE可视化潜在空间')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig('tsne_visualization.png', dpi=300)
    plt.show()

def visualize_attention_weights(X_test, get_attention_weights, model):
    """
    可视化注意力权重
    
    参数:
    - X_test: 测试数据
    - get_attention_weights: 获取注意力权重的函数
    - model: 训练好的模型
    """
    # 从测试集选择几个样本
    sample_indices = np.random.choice(len(X_test), 3, replace=False)
    samples = X_test[sample_indices]
    
    # 获取注意力权重
    attention_weights = []
    
    # 假设模型接收样本作为输入，返回注意力权重
    # 这里需要根据您的模型架构进行调整
    for sample in samples:
        # 扩展维度以匹配模型输入
        sample_expanded = np.expand_dims(sample, axis=0)
        
        # 预测并获取注意力权重
        # 这里假设您的模型有一个方法可以返回注意力权重
        _, attention = model.predict(sample_expanded)
        attention_weights.append(attention[0])  # 假设返回的attention是批量的
    
    # 可视化注意力权重和原始信号
    plt.figure(figsize=(15, 10))
    
    for i, (sample, attention) in enumerate(zip(samples, attention_weights)):
        # 计算原始信号的峰值，用于绘图标准化
        signal_peak = np.max(np.abs(sample))
        
        # 绘制原始信号 (X轴)
        plt.subplot(3, 1, i+1)
        
        # 绘制X轴
        plt.plot(sample[:, 0], 'b-', label='X轴信号', alpha=0.7)
        
        # 绘制Y轴
        plt.plot(sample[:, 1], 'g-', label='Y轴信号', alpha=0.7)
        
        # 绘制Z轴
        plt.plot(sample[:, 2], 'r-', label='Z轴信号', alpha=0.7)
        
        # 绘制注意力权重（根据原始信号的峰值进行缩放）
        plt.plot(attention * signal_peak, 'k-', label='注意力权重', linewidth=2)
        
        plt.title(f'样本 {i+1}')
        plt.xlabel('时间步')
        plt.ylabel('幅度')
        plt.legend()
    
    plt.tight_layout()
    plt.savefig('attention_visualization.png', dpi=300)
    plt.show()

def save_model_and_metadata(model, label_map, mean, std, window_size):
    """
    保存模型和元数据
    
    参数:
    - model: 训练好的模型
    - label_map: 标签映射
    - mean: 均值（用于归一化）
    - std: 标准差（用于归一化）
    - window_size: 窗口大小
    """
    # 保存模型
    model.save('bearing_fault_model.h5')
    
    # 保存标签映射
    np.save('label_map.npy', label_map)
    
    # 保存归一化参数
    np.save('normalization_mean.npy', mean)
    np.save('normalization_std.npy', std)
    
    # 保存窗口大小
    with open('model_metadata.txt', 'w') as f:
        f.write(f"window_size: {window_size}\n")
        f.write(f"num_classes: {len(label_map)}\n")
    
    print("模型和元数据已保存")

def load_model_and_predict(model_path, file_path, label_map_path, mean_path, std_path, window_size=1000):
    """
    加载模型并预测新数据
    
    参数:
    - model_path: 模型路径
    - file_path: 新数据文件路径
    - label_map_path: 标签映射路径
    - mean_path: 均值路径
    - std_path: 标准差路径
    - window_size: 窗口大小
    
    返回:
    - 预测结果
    """
    # 加载模型
    model = tf.keras.models.load_model(model_path)
    
    # 加载标签映射
    label_map = np.load(label_map_path, allow_pickle=True).item()
    reverse_label_map = {v: k for k, v in label_map.items()}
    
    # 加载归一化参数
    mean = np.load(mean_path)
    std = np.load(std_path)
    
    # 加载并处理新数据
    data = load_and_process_data(file_path)
    segments = segment_data(data, window_size)
    
    # 归一化
    normalized_segments = (segments - mean) / std
    
    # 预测
    predictions = model.predict(normalized_segments)
    predicted_classes = np.argmax(predictions, axis=1)
    
    # 统计预测结果
    class_counts = np.bincount(predicted_classes)
    most_common_class = np.argmax(class_counts)
    
    # 获取最终预测类别名称
    final_prediction = reverse_label_map[most_common_class]
    
    # 计算置信度
    confidence = class_counts[most_common_class] / len(predicted_classes)
    
    print(f"预测结果: {final_prediction}")
    print(f"置信度: {confidence:.2f}")
    
    # 可视化各段预测结果
    plt.figure(figsize=(12, 6))
    plt.bar(range(len(class_counts)), class_counts)
    plt.xticks(range(len(class_counts)), [reverse_label_map.get(i, f"Class {i}") for i in range(len(class_counts))], rotation=90)
    plt.title('各类别预测数量')
    plt.xlabel('类别')
    plt.ylabel('数量')
    plt.tight_layout()
    plt.show()
    
    return final_prediction, confidence

# 主函数
if __name__ == "__main__":
    # 示例：训练并评估模型
    # data_dir = "bearing_data_directory"
    # model, history, (loss, accuracy) = train_and_evaluate_model(data_dir)
    
    # 示例：保存模型和元数据
    # save_model_and_metadata(model, label_map, mean, std, window_size=1000)
    
    # 示例：加载模型并预测新数据
    # prediction, confidence = load_model_and_predict(
    #     'bearing_fault_model.h5',
    #     'new_data.csv',
    #     'label_map.npy',
    #     'normalization_mean.npy',
    #     'normalization_std.npy'
    # )
    
    print("示例代码完成")