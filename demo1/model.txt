import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (
    Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten, 
    Input, Multiply, Permute, Activation, Lambda, 
    GlobalAveragePooling1D, Reshape, RepeatVector, BatchNormalization,
    TimeDistributed, Bidirectional, Layer
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K

class SelfAttention(Layer):
    """自定义自注意力层"""
    def __init__(self, attention_units=32, **kwargs):
        self.attention_units = attention_units
        super(SelfAttention, self).__init__(**kwargs)
        
    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], self.attention_units),
                                 initializer="glorot_uniform", trainable=True)
        self.b = self.add_weight(name="att_bias", shape=(self.attention_units,),
                                 initializer="zeros", trainable=True)
        
        self.u = self.add_weight(name="att_context", shape=(self.attention_units, 1),
                                 initializer="glorot_uniform", trainable=True)
        super(SelfAttention, self).build(input_shape)
    
    def call(self, x):
        # x shape: (batch_size, time_steps, features)
        # 计算注意力权重
        ui = K.tanh(K.dot(x, self.W) + self.b)  # (batch_size, time_steps, units)
        ai = K.dot(ui, self.u)  # (batch_size, time_steps, 1)
        ai = K.squeeze(ai, axis=-1)  # (batch_size, time_steps)
        ai = K.softmax(ai)  # (batch_size, time_steps)
        
        # 应用注意力权重
        attention_weights = K.expand_dims(ai, axis=-1)  # (batch_size, time_steps, 1)
        weighted_output = x * attention_weights  # (batch_size, time_steps, features)
        
        # 返回加权和和注意力权重
        return K.sum(weighted_output, axis=1), ai
    
    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[2]), (input_shape[0], input_shape[1])

def create_cnn_lstm_attention_model(input_shape, num_classes, filters=64, kernel_size=3, 
                                    lstm_units=100, dropout_rate=0.3):
    """
    创建1D CNN + LSTM + 自注意力机制的模型
    
    参数:
    - input_shape: 输入数据的形状，例如(1000, 3)表示1000个时间步长，3个特征(X,Y,Z轴)
    - num_classes: 分类数量，例如轴承故障类型数量
    - filters: CNN卷积核数量
    - kernel_size: CNN卷积核大小
    - lstm_units: LSTM单元数量
    - dropout_rate: Dropout比率
    
    返回:
    - 完整的Keras模型
    """
    # 输入层
    inputs = Input(shape=input_shape)
    
    # 1D CNN部分 - 提取局部特征
    x = Conv1D(filters, kernel_size, activation='relu', padding='same')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling1D(pool_size=2)(x)
    
    x = Conv1D(filters*2, kernel_size, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling1D(pool_size=2)(x)
    
    x = Conv1D(filters*4, kernel_size, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    
    # LSTM部分 - 捕获时序依赖关系
    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)
    x = Dropout(dropout_rate)(x)
    
    # 自注意力机制 - 关注重要特征
    att_output, att_weights = SelfAttention(attention_units=64)(x)
    
    # 全连接层 - 分类
    x = Dense(128, activation='relu')(att_output)
    x = Dropout(dropout_rate)(x)
    x = Dense(64, activation='relu')(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    
    # 创建模型
    model = Model(inputs=inputs, outputs=outputs)
    
    # 编译模型
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model, att_weights

def prepare_data_for_model(data, window_size=1000, step=500, num_classes=38):
    """
    准备数据用于模型训练
    
    参数:
    - data: 原始数据，假设格式为(samples, 4)，其中第一列是时间戳，后三列是X,Y,Z轴
    - window_size: 窗口大小
    - step: 滑动窗口步长
    - num_classes: 分类数量
    
    返回:
    - X: 特征数据
    - y: 标签
    """
    # 提取特征 (假设前三列是时间戳，后三列是X,Y,Z轴)
    features = data[:, 1:4]  # 提取X,Y,Z轴数据
    
    # 创建滑动窗口
    X = []
    for i in range(0, len(features) - window_size + 1, step):
        X.append(features[i:i + window_size])
    
    X = np.array(X)
    
    # 这里假设每个数据集对应一个类别
    # 在实际应用中，你需要根据数据集的具体情况来设置标签
    # 例如，可以根据故障类型和严重程度来分类
    y = np.zeros((len(X), num_classes))  # 独热编码标签
    
    # 这里只是一个示例，实际应用中需要根据具体数据设置标签
    # 例如，如果当前数据是"0.7mm内圈故障，100W负载"，可能对应类别0
    # 如果是"0.9mm外圈故障，200W负载"，可能对应类别1
    # 以此类推...
    
    return X, y

def train_model(model, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):
    """
    训练模型
    
    参数:
    - model: 创建的模型
    - X_train, y_train: 训练数据
    - X_val, y_val: 验证数据
    - epochs: 训练轮数
    - batch_size: 批次大小
    
    返回:
    - 训练历史
    """
    # 早停法
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    )
    
    # 学习率衰减
    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6
    )
    
    # 训练模型
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, lr_scheduler]
    )
    
    return history

def evaluate_model(model, X_test, y_test):
    """
    评估模型
    
    参数:
    - model: 训练好的模型
    - X_test, y_test: 测试数据
    
    返回:
    - 测试损失和准确率
    """
    # 评估模型
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"Test Loss: {loss:.4f}")
    print(f"Test Accuracy: {accuracy:.4f}")
    
    # 预测
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_test, axis=1)
    
    # 计算混淆矩阵
    from sklearn.metrics import confusion_matrix, classification_report
    cm = confusion_matrix(y_true_classes, y_pred_classes)
    
    # 打印分类报告
    print("\nClassification Report:")
    print(classification_report(y_true_classes, y_pred_classes))
    
    return loss, accuracy, cm

# 示例：如何使用上面的函数
if __name__ == "__main__":
    # 1. 读取数据
    # 这里假设你已经有了数据，格式为(samples, 4)，其中第一列是时间戳，后三列是X,Y,Z轴
    # data = pd.read_csv("your_data.csv").values
    
    # 2. 准备数据
    # X, y = prepare_data_for_model(data)
    
    # 3. 划分训练、验证和测试集
    # from sklearn.model_selection import train_test_split
    # X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
    # X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
    
    # 4. 创建模型
    # input_shape = (1000, 3)  # 1000个时间步长，3个特征(X,Y,Z轴)
    # num_classes = 38  # 38个轴承状态
    # model, _ = create_cnn_lstm_attention_model(input_shape, num_classes)
    
    # 5. 训练模型
    # history = train_model(model, X_train, y_train, X_val, y_val)
    
    # 6. 评估模型
    # loss, accuracy, cm = evaluate_model(model, X_test, y_test)
    
    # 7. 保存模型
    # model.save("bearing_fault_model.h5")
    
    print("示例代码完成")